# Running Spark Applications and Command-Line Configurations

In this section, we'll cover how to run Spark applications, explore the key configurations available in `spark-submit`, and discuss the interactive shells (`pyspark` and `spark-shell`) provided by Apache Spark.
___
## 1. Running Spark Applications with `spark-submit`
The `spark-submit` script is the primary way to launch applications in Spark. It allows you to submit your application to a cluster for execution.

### Basic `spark-submit` Command
To submit a Spark application, you can use the following command:
```bash
spark-submit --class <main-class> --master <master-url> <application-jar> [application-arguments]
```

 - `--class <main-class>`: Specifies the main class of your application (only for Java/Scala).
 - `--master <master-url>`: Defines the cluster manager to connect to (e.g., `yarn`, `local`, `spark://HOST:PORT`).
 - `<application-jar>`: The path to your application JAR file.
 - `[application-arguments]`: Optional arguments to pass to your main class.

### Example: Submitting a Scala/Java Application
```bash
spark-submit --class org.example.MyApp --master yarn --deploy-mode cluster myapp.jar input.txt output/
```
Where:
 - `--deploy-mode cluster`: Runs the driver on the cluster -- we discuss the most important command line configurations in length in the next paragraphs. 
 - ` myapp.jar`: The compiled JAR file of your Spark application.
 - `input.txt output/`: Arguments passed to the application.

### Example: Submitting a Python Application
```bash
spark-submit --master yarn --deploy-mode cluster myscript.py input.txt output/
```
Where:
 - `myscript.py`: Your Python script.

### Key `spark-submit` Options
- `--master`: Specifies the master URL for the cluster.
    - `yarn` for running on YARN.
    - `local[N]` for running locally with N cores.
- `--deploy-mode`: Defines where to run the driver program.
  - `client`: Driver runs on the client machine.
  - `cluster`: Driver runs on the cluster.
- `--num-executors`: Number of executors to launch.
   - Example: `--num-executors 10`
- `--executor-memory`: Memory per executor.
   - Example: `--executor-memory 4G`
- `--executor-cores`: Number of cores per executor.
   - Example: `--executor-cores 2`
- `--driver-memory`: Memory for the driver.
   - Example: `--driver-memory 2G`
- `--conf`: Allows setting specific Spark configurations.
   - Example: `--conf spark.executor.memoryOverhead=512`
- `--jars`: Comma-separated list of JARs to include on the driver and executor classpaths.
   - Example: `--jars additional.jar,dependency.jar`
- `--files`: Comma-separated list of files to be distributed with the job.
   - Example: `--files config.properties`
- `--py-files`: For Python, you can include additional Python files.
   - Example: `--py-files dependencies.zip`
___
 
## 2. Interactive Shells in Spark
Spark provides two interactive shells that allow you to run Spark commands in real-time: `spark-shell` (Scala) and `pyspark` (Python).

### a) `spark-shell` (Scala)

`spark-shell` is a REPL (Read-Eval-Print Loop) for Spark, where you can run Spark commands using Scala.

To start spark-shell, simply run:
```bash
spark-shell
```
You can specify options similar to `spark-submit`:
```bash
spark-shell --master yarn --executor-memory 4G --num-executors 4
```
### b) `pyspark` (Python)
`pyspark` is the Python counterpart to `spark-shell`, allowing you to interact with Spark using Python.
To start pyspark, run:
```bash
pyspark
```
You can also pass in options:
```bash
pyspark --master yarn --executor-memory 4G --num-executors 4
```
___
## 3. Conclusion

By mastering the `spark-submit` command and becoming comfortable with the interactive shells, you'll be well-equipped to develop, test, and deploy Spark applications in a distributed environment. Whether you're writing in Scala or Python, Spark's tools make it easy to experiment and iterate quickly, providing a powerful platform for big data analytics.