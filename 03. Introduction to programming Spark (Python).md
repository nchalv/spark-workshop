## Preliminaries: Uploading data on HDFS

First, clone the repository to the controller local filesystem.
```bash
git clone https://github.com/nchalv/spark-workshop.git
```
The command which allows us to copy local files to HDFS is the following:
```bash
hdfs dfs -put <localsrc> <dest>
```
Use it to copy the files in the `data/examples` directory to HDFS:
```bash
cd spark-workshop
hdfs dfs -put ./data/ /user/ubuntu/
```


## Example 1 - Wordcount (Plain Map/Reduce)

**Wordcount**: find the number of occurences of each word in a body of text.

#### Data format
The file `text.txt` contains the following short text:

```
this is a text file with random words like text , words , like this is an example of a text file
```

The data is stored in HDFS: `hdfs://controller:54310/user/ubuntu/data/examples`.


#### How Map/Reduce works


<div align="center">
	<img src="https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212104&authkey=%21ANOdfNvhzGtab5E&width=660">
</div>


#### Code

```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("wordcount example") \
    .getOrCreate() \
    .sparkContext

wordcount = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/text.txt") \
    .flatMap(lambda x: x.split(" ")) \
    .map(lambda x: (x, 1)) \
    .reduceByKey(lambda x,y: x+y) \
    .sortBy(lambda x: x[1], ascending=False)

print(wordcount.collect())
```
#### Output
```
[('text', 3), ('this', 2), ('is', 2), ('like', 2), ('a', 2), ('file', 2), ('words', 2), (',', 2), ('an', 1), ('of', 1), ('with', 1), ('random', 1), ('example', 1)]
```
#### Code explanation
- First, we create a `sparkSession` and a `SparkContex`:
    - `sparkSession` is an entry point for every programming library in Spark and we need to create one in order to execute code.
    - The `sparkContext` is an entry point specific for RDDs.

- Then, the program reads the `text.txt` file from HDFS. With the use of a `lambda function` we split the data every time there is a whitespace between them.
    - A lambda function is essentially an anonymous function we can use to write quick throwaway functions without defining or naming them.
    - The lambda function the program uses as a `flatMap` argument: `Lambda x: x.split(" ")`
    - `flatMap` vs `map`: instead of creating multiple lists -> single list with all values.

- Next, with the use of a `map` function we create a `(key,value)` pair for every word.
    - We set `key = $word` and `value = 1`

- We use the `reduceByKey` function: every tuple with the same `key` is sent to the same `reducer` so it **aggregate** them and create the result.
    - In our case, if more than one tuples with the same `key` exist,  we **add** their `values`

- Finally, we `sortBy` `value` (number of occurrence) and print the result.


## Example 2 - Simple Database
### Data format
`employees.csv` contains the ID of the employee, the name of the employee, the salary of the employee and the ID of the department that the employee works at.

| ID          | Name        | Salary      | DepartmentID |
| ----------- | ----------- | ----------- | ------------ |
| 1           | George R    | 2000        | 1            |

`departments.csv` contains the ID of the department and the name of the department.

| ID          | Name        |
| ----------- | ----------- |
| 1           | Dep A       |

The data is stored in HDFS: `hdfs://controller:54310/user/ubuntu/data/examples`.
### QUERY 1: Find the 5 worst paid employees
##### Code - RDD API
```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 1 execution") \
    .getOrCreate() \
    .sparkContext
    
employees = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv") \
                .map(lambda x: (x.split(",")))
sorted_employees = employees.map(lambda x: [int(x[2]), [x[0], x[1], x[3]] ]) \
                    .sortByKey()                    
print(sorted_employees.take(5))
```
##### Output
```
[(550, ['6', 'Jerry L', '3']), (1000, ['2', 'John K', '2']), (1000, ['7', 'Victor K', '1']), (1050, ['5', 'Helen K', '2']), (1500, ['10', 'Fiona T', '1'])]
```
Try to explain:
-  `flatMmap` vs `map`: why `map` here?
- what does the second `lambda` function do?

##### Code - DataFrame API
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col

spark = SparkSession \
    .builder \
    .appName("DF query 1 execution") \
    .getOrCreate()

employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

employees_df = spark.read.csv("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv", header=False, schema=employees_schema)
employees_df.printSchema()

## Alternative way to read csv:
# employees_df = spark.read.format('csv') \
#     .options(header='false') \
#     .schema(employees_schema) \
#     .load("hdfs://okeanos-master:54310/user/ubuntu/ta/advanced-db/spark-examples/employees.csv")

sorted_empolpyess_df = employees_df.sort(col("salary"))
## Use "explain()" to display physical plan:
# sorted_empolpyess_df.explain(mode="formatted")
sorted_empolpyess_df.show(5)
```

##### Output
```
+---+---------+------+------+
| id|     name|salary|dep_id|
+---+---------+------+------+
|  6|  Jerry L| 550.0|     3|
|  2|   John K|1000.0|     2|
|  7| Victor K|1000.0|     1|
|  5|  Helen K|1050.0|     2|
| 10|  Fiona T|1500.0|     1|
+---+---------+------+------+
only showing top 5 rows
```

Remember to use `explain()` to check if the physical plan is what you expect.

#### QUERY 2: Find the 3 best paid employees from "Dep A"
##### Code - RDD API
```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 2 execution") \
    .getOrCreate() \
    .sparkContext

employees = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv") \
                .map(lambda x: (x.split(",")))
departments = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/departments.csv") \
                .map(lambda x: (x.split(",")))
depA = departments.map(lambda x: x if (x[1] == "Dep A") else None) \
                .filter(lambda x: x != None)
# print(depA.collect())


employees_formatted = employees.map(lambda x: [x[3] , [x[0],x[1],x[2]] ] )
depA_formatted = depA.map(lambda x: [x[0], [x[1]]])
# print(employees_formatted.collect())
# print(depA_formatted.collect())

joined_data = employees_formatted.join(depA_formatted)
# print(joined_data.collect())

get_employees = joined_data.map(lambda x: (x[1][0]))
# print(get_employees.collect())

sorted_employees= get_employees.map(lambda x: [int(x[2]),[x[0], x[1]] ] ) \
                                .sortByKey(ascending=False)
print(sorted_employees.take(3))
```

##### Output
```
[(2100, ['3', 'Mary T']), (2100, ['4', 'George T']), (2000, ['1', 'George R'])]
```
##### Code - SQL API
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
spark = SparkSession \
    .builder \
    .appName("DF query 2 execution") \
    .getOrCreate()

employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv")

departments_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
])

departments_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(departments_schema) \
    .load("hdfs://controller:54310/user/ubuntu/data/examples/departments.csv")

# To utilize as SQL tables
employees_df.createOrReplaceTempView("employees")
departments_df.createOrReplaceTempView("departments")

id_query = "SELECT departments.id, departments.name \
    FROM departments \
    WHERE departments.name == 'Dep A'"

depA_id = spark.sql(id_query)
depA_id.createOrReplaceTempView("depA")
inner_join_query = "SELECT employees.name, employees.salary \
    FROM employees \
    INNER JOIN depA ON employees.dep_id == depA.id \
    ORDER BY employees.salary DESC"

joined_data = spark.sql(inner_join_query)
joined_data.show(3)
```


##### Output
```
+--------+------+
|    name|salary|
+--------+------+
|  Mary T|2100.0|
|George T|2100.0|
|George R|2000.0|
+--------+------+
only showing top 3 rows
```

### Example 3 - Simple Database with a twist (DataFrame UDFs)

Sometimes we need to define functions that process the values of specific columns of a single row.

Motivating example: a database with salaries and bonuses for our employees:

| ID          | Name        | Salary      | DepartmentID | Bonus        |
| ----------- | ----------- | ----------- | ------------ | ------------ |
| 1           | George R    | 2000        | 1            | 500          |

We wan to calculate the total yearly income for each one of them: `14 x Salary + Bonus`

#### Code - RDD API: the `map` function is enough
```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 1 execution") \
    .getOrCreate() \
    .sparkContext
    
employees = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/employees2.csv") \
                .map(lambda x: (x.split(",")))
employees_yearly = employees.map(lambda x: [x[1], 14*(int(x[2]))+int(x[4])])
print(employees_yearly.collect())
```

#### Output
```
[['George R', 28500], ['John K', 14150], ['Mary T', 29850], ['George T', 29720], ['Helen K', 14900], ['Jerry L', 7900], ['Victor K', 14550], ['George K', 36500], ['Miranda D', 50000], ['Fiona T', 21450], ['Tony T', 35620]]
```

#### Code - DataFrame API (UDF)
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col, udf

spark = SparkSession.builder \
    .appName("UDF example") \
    .getOrCreate()

employees2_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
    StructField("bonus", FloatType()),
])

def calculate_yearly_income(salary, bonus):
    return 14*salary+bonus

# Register the UDF
calculate_yearly_income_udf = udf(calculate_yearly_income, FloatType())

employees_df = spark.read.csv("hdfs://controller:54310/user/ubuntu/data/examples/employees2.csv", header=False, schema=employees2_schema)

employees_yearly_df = employees_df \
    .withColumn("yearly", calculate_yearly_income_udf(col("salary"), col("bonus"))).select("name", "yearly")

employees_yearly_df.show()
```

#### Output
```
+----------+-------+
|      name| yearly|
+----------+-------+
|  George R|28500.0|
|    John K|14150.0|
|    Mary T|29850.0|
|  George T|29720.0|
|   Helen K|14900.0|
|   Jerry L| 7900.0|
|  Victor K|14550.0|
|  George K|36500.0|
| Miranda D|50000.0|
|   Fiona T|21450.0|
|    Tony T|35620.0|
+----------+-------+
```
