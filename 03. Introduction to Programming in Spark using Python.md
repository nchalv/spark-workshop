# Introduction to Programming in Spark using Python
## Why Python?
Python is an excellent choice as the first programming interface for an introduction to Apache Spark due to its simplicity and ease of use compared to Java or Scala. Setting up and compiling Java or Scala projects typically involves complex build configurations, dependency management, and lengthy compilation times, which can introduce significant overhead and steep learning curves for newcomers (`mvn`, `sbt`). Python, on the other hand, offers a more straightforward setup with its high-level syntax and dynamic typing, allowing users to write and execute Spark code with minimal boilerplate. Its interactive environment and rich ecosystem of libraries facilitate rapid experimentation and prototyping, making it an ideal choice for beginners to quickly grasp Spark's core concepts and data processing capabilities without getting bogged down by the intricacies of JVM-based languages.

___
## Uploading data on HDFS
To execute the following examples, you will need to have completed the configuration steps described in the guide: [**00. Apache Spark over YARN installation guide**](https://github.com/nchalv/spark-workshop/blob/main/00.%20Apache%20Spark%20over%20YARN%20installation%20guide.md).
You will then need to upload the data files included in the examples directory to HDFS. 
First, clone the repository to the controller local filesystem.
<details>
  <summary>Click to reveal the command.</summary>
  <pre><code>git clone https://github.com/nchalv/spark-workshop.git</code></pre>
</details>

To avoid repeating this process, also download the main data set required for [04. Advanced Analytics on Los Angeles Crime Data](https://github.com/nchalv/spark-workshop/blob/main/04.%20Advanced%20Analytics%20on%20Los%20Angeles%20Crime%20Data.md).
<details>
<summary>Click to reveal the command.</summary>
<pre><code>wget https://data.lacity.org/api/views/63jg-8b9z/rows.csv?accessType=DOWNLOAD -O ./spark-workshop/data/la_crime_data/CrimeData_10-19.csv</code></pre>
</details>

Finally, use the appropriate command according to guide:  [**01. Basic Command-Line Operations on HDFS**](https://github.com/nchalv/spark-workshop/blob/main/01.%20Basic%20HDFS%20operations.md) to copy the files in the `data` directory to HDFS.
<details>
<summary>Click to reveal the commands.</summary>
<pre><code>cd spark-workshop
hdfs dfs -put ./data/ /user/ubuntu/</code></pre>
</details>

___
## Tips
 - Try to execute the code given below using both `spark-submit` as well as line-by-line using the interactive shell (`pyspark`).
 - Utilise the interactive feature of the latter to check intermediate results of the operators we use to better understand the logic of each application.
 - Experiment! Nothing could go wrong!
___

## Example 1 - Wordcount (Plain Map/Reduce)

**Wordcount**: *find the number of occurrences of each word in a body of text*.

### Data format
The file `text.txt` contains the following short text:

```
this is a text file with random words like text , words , like this is an example of a text file
```

The data is stored in HDFS: `hdfs://controller:54310/user/ubuntu/data/examples`.


### How Map/Reduce works


<div align="center">
	<img src="https://onedrive.live.com/embed?resid=23805F5DB37ABB76%212104&authkey=%21ANOdfNvhzGtab5E&width=660">
</div>

### Code

```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("wordcount example") \
    .getOrCreate() \
    .sparkContext

wordcount = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/text.txt") \
    .flatMap(lambda x: x.split(" ")) \
    .map(lambda x: (x, 1)) \
    .reduceByKey(lambda x,y: x+y) \
    .sortBy(lambda x: x[1], ascending=False)

print(wordcount.collect())
```
### Output

<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>[('text', 3), ('this', 2), ('is', 2), ('like', 2), ('a', 2), ('file', 2), ('words', 2), (',', 2), ('an', 1), ('of', 1), ('with', 1), ('random', 1), ('example', 1)]</code></pre>
</details>

### Commentary
- First, we create a `sparkSession` and a `SparkContex`:
    - `sparkSession` is an entry point for every programming library in Spark and we need to create one in order to execute code.
    - The `sparkContext` is an entry point specific for RDDs.

- Then, the program reads the `text.txt` file from HDFS. With the use of a `lambda function` we split the data every time there is a whitespace between them.
    - A lambda function is essentially an anonymous function we can use to write quick throwaway functions without defining or naming them.
    - The lambda function the program uses as a `flatMap` argument: `Lambda x: x.split(" ")`
    - `flatMap` vs `map`: instead of creating multiple lists -> single list with all values.

- Next, with the use of a `map` function we create a `(key,value)` pair for every word.
    - We set `key = $word` and `value = 1`

- We use the `reduceByKey` function: every tuple with the same `key` is sent to the same `reducer` so it **aggregate** them and create the result.
    - In our case, if more than one tuples with the same `key` exist,  we **add** their `values`

- Finally, we `sortBy` `value` (number of occurrences) and print the result.

___
## Example 2 - Simple Database
### Data format
`employees.csv` contains the ID of the employee, the name of the employee, the salary of the employee and the ID of the department that the employee works at.

| ID          | Name        | Salary      | DepartmentID |
| ----------- | ----------- | ----------- | ------------ |
| 1           | George R    | 2000        | 1            |

`departments.csv` contains the ID of the department and the name of the department.

| ID          | Name        |
| ----------- | ----------- |
| 1           | Dep A       |

The data is stored in HDFS: `hdfs://controller:54310/user/ubuntu/data/examples`.
### QUERY 1: *Find the 5 worst paid employees*
### Code - RDD API
```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 1 execution") \
    .getOrCreate() \
    .sparkContext
    
employees = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv") \
                .map(lambda x: (x.split(",")))
sorted_employees = employees.map(lambda x: [int(x[2]), [x[0], x[1], x[3]] ]) \
                    .sortByKey()                    
print(sorted_employees.take(5))
```
### Output
<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>[(550, ['6', 'Jerry L', '3']), (1000, ['2', 'John K', '2']), (1000, ['7', 'Victor K', '1']), (1050, ['5', 'Helen K', '2']), (1500, ['10', 'Fiona T', '1'])]</code></pre>
</details>

Try to explain:
-  `flatMmap` vs `map`: why `map` here?
- what does the second `lambda` function do?

___

### Code - DataFrame API
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col

spark = SparkSession \
    .builder \
    .appName("DF query 1 execution") \
    .getOrCreate()

employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

employees_df = spark.read.csv("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv", header=False, schema=employees_schema)
employees_df.printSchema()

## Alternative way to read csv:
# employees_df = spark.read.format('csv') \
#     .options(header='false') \
#     .schema(employees_schema) \
#     .load("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv")

sorted_empolpyess_df = employees_df.sort(col("salary"))
## Use "explain()" to display physical plan:
# sorted_empolpyess_df.explain(mode="formatted")
sorted_empolpyess_df.show(5)
```

### Output
<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>+---+---------+------+------+
| id|     name|salary|dep_id|
+---+---------+------+------+
|  6|  Jerry L| 550.0|     3|
|  2|   John K|1000.0|     2|
|  7| Victor K|1000.0|     1|
|  5|  Helen K|1050.0|     2|
| 10|  Fiona T|1500.0|     1|
+---+---------+------+------+
only showing top 5 rows</code></pre>
</details>


Remember to use `explain()` to check if the physical plan is what you expect.
___

### QUERY 2: *Find the 3 best paid employees from "Dep A"*
### Code - RDD API
```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 2 execution") \
    .getOrCreate() \
    .sparkContext

employees = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv") \
                .map(lambda x: (x.split(",")))
departments = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/departments.csv") \
                .map(lambda x: (x.split(",")))
depA = departments.map(lambda x: x if (x[1] == "Dep A") else None) \
                .filter(lambda x: x != None)
# print(depA.collect())


employees_formatted = employees.map(lambda x: [x[3] , [x[0],x[1],x[2]] ] )
depA_formatted = depA.map(lambda x: [x[0], [x[1]]])
# print(employees_formatted.collect())
# print(depA_formatted.collect())

joined_data = employees_formatted.join(depA_formatted)
# print(joined_data.collect())

get_employees = joined_data.map(lambda x: (x[1][0]))
# print(get_employees.collect())

sorted_employees= get_employees.map(lambda x: [int(x[2]),[x[0], x[1]] ] ) \
                                .sortByKey(ascending=False)
print(sorted_employees.take(3))
```

### Output
<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>[(2100, ['3', 'Mary T']), (2100, ['4', 'George T']), (2000, ['1', 'George R'])]</code></pre>
</details>

___
### Code - SQL API
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
spark = SparkSession \
    .builder \
    .appName("DF query 2 execution") \
    .getOrCreate()

employees_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
])

employees_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(employees_schema) \
    .load("hdfs://controller:54310/user/ubuntu/data/examples/employees.csv")

departments_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
])

departments_df = spark.read.format('csv') \
    .options(header='false') \
    .schema(departments_schema) \
    .load("hdfs://controller:54310/user/ubuntu/data/examples/departments.csv")

# To utilize as SQL tables
employees_df.createOrReplaceTempView("employees")
departments_df.createOrReplaceTempView("departments")

id_query = "SELECT departments.id, departments.name \
    FROM departments \
    WHERE departments.name == 'Dep A'"

depA_id = spark.sql(id_query)
depA_id.createOrReplaceTempView("depA")
inner_join_query = "SELECT employees.name, employees.salary \
    FROM employees \
    INNER JOIN depA ON employees.dep_id == depA.id \
    ORDER BY employees.salary DESC"

joined_data = spark.sql(inner_join_query)
joined_data.show(3)
```

### Output
<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>+--------+------+
|    name|salary|
+--------+------+
|  Mary T|2100.0|
|George T|2100.0|
|George R|2000.0|
+--------+------+
only showing top 3 rows</code></pre>
</details>

___
## Example 3 - Simple Database with a twist (DataFrame UDFs)

Sometimes we need to define functions that process the values of specific columns of a single row.

Motivating example: a database with salaries and bonuses for the employees:

| ID          | Name        | Salary      | DepartmentID | Bonus        |
| ----------- | ----------- | ----------- | ------------ | ------------ |
| 1           | George R    | 2000        | 1            | 500          |

We want to calculate the total yearly income for each one of them, assuming they all received their bonuses: `12 x Salary + Bonus`.

### Code - RDD API: the `map` function is enough
```python
from pyspark.sql import SparkSession

sc = SparkSession \
    .builder \
    .appName("RDD query 1 execution") \
    .getOrCreate() \
    .sparkContext
    
employees = sc.textFile("hdfs://controller:54310/user/ubuntu/data/examples/employees2.csv") \
                .map(lambda x: (x.split(",")))
employees_yearly = employees.map(lambda x: [x[1], 12*(int(x[2]))+int(x[4])])
print(employees_yearly.collect())
```

### Output
<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>[['George R', 28500], ['John K', 14150], ['Mary T', 29850], ['George T', 29720], ['Helen K', 14900], ['Jerry L', 7900], ['Victor K', 14550], ['George K', 36500], ['Miranda D', 50000], ['Fiona T', 21450], ['Tony T', 35620]]</code></pre>
</details>

___
### Code - DataFrame API (UDF)
```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType
from pyspark.sql.functions import col, udf

spark = SparkSession.builder \
    .appName("UDF example") \
    .getOrCreate()

employees2_schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("salary", FloatType()),
    StructField("dep_id", IntegerType()),
    StructField("bonus", FloatType()),
])

def calculate_yearly_income(salary, bonus):
    return 14*salary+bonus

# Register the UDF
calculate_yearly_income_udf = udf(calculate_yearly_income, FloatType())

employees_df = spark.read.csv("hdfs://controller:54310/user/ubuntu/data/examples/employees2.csv", header=False, schema=employees2_schema)

employees_yearly_df = employees_df \
    .withColumn("yearly", calculate_yearly_income_udf(col("salary"), col("bonus"))).select("name", "yearly")

employees_yearly_df.show()
```

### Output
<details>
  <summary>Click to reveal the result.</summary>
  <pre><code>+----------+-------+
|      name| yearly|
+----------+-------+
|  George R|28500.0|
|    John K|14150.0|
|    Mary T|29850.0|
|  George T|29720.0|
|   Helen K|14900.0|
|   Jerry L| 7900.0|
|  Victor K|14550.0|
|  George K|36500.0|
| Miranda D|50000.0|
|   Fiona T|21450.0|
|    Tony T|35620.0|
+----------+-------+</code></pre>
</details>

___